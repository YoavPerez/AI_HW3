import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
# import pickle
import math
import time

train_df = pd.read_csv("./data/train.csv")
test_df = pd.read_csv("./data/test.csv")

classes = set(train_df["diagnosis"])

class Node:
    """Contains the information of the node and another nodes of the Decision Tree."""
    def __init__(self, type, val, children, num_of_samples, data):
        self.type = type
        self.split_val = val
        self.children = children
        self.num_of_samples = num_of_samples
        self.data = data


class Leaf:
    """Contains the diagnosis of the leaf."""
    def __init__(self, diag, data):
        self.diag = diag
        self.data = data


def calc_entropy(diags, total_size):
    if total_size == 0:
        return 0
        print("NO NO NO NO NO!!!")

    entropy = 0
    num_m = np.count_nonzero(diags == 'M')
    p_m = num_m / total_size
    p_b = (total_size - num_m) / total_size

    for p_c in [p_m, p_b]:
        if p_c == 0:
            continue
        entropy -= p_c * np.log2(p_c)
    return entropy


def calc_IG(train_df, df1, df2, total_size, len1, len2):
    """ Return the information gain. """
    IG = calc_entropy(train_df, total_size)
    IG -= (len1 / total_size) * calc_entropy(df1, len1)
    IG -= (len2 / total_size) * calc_entropy(df2, len2)
    return IG


def choose_value(train_df, attribute, total_size, useLoss=False):
    """ Find the best splitting value of the attribute. Returns the best value and the information gain """
    best_ig = -math.inf
    best_val = None

    list_of_new_qualities = train_df.sort_values(by=[attribute])[attribute].unique()
    prev = None

    for val in list_of_new_qualities:
        if prev == None:
            prev = val
            continue

        avg = (val + prev) / 2
        vals = train_df[attribute].to_numpy(copy=True)  # the values of the feature, already sorted
        diags = train_df["diagnosis"].to_numpy(copy=True)  # the diagnostics of the data
        first_son = np.where(vals >= val)
        second_son = np.where(vals < val)
        df1 = diags[first_son]
        df2 = diags[second_son]
        len1 = np.size(df1)
        len2 = np.size(df2)
        tmp_ig = calc_IG(diags, df1, df2, total_size, len1, len2)

        if tmp_ig >= best_ig:
            best_ig = tmp_ig
            best_val = avg
        prev = val

    return (best_val, best_ig)


def choose_quality(train_df, qualities, total_size):
    """ Choose the quality that splits the best. Return the quality and the value to split from. """
    best_q = None
    q_val = None
    best_IG = -math.inf
    for q in qualities:
        # tmp_q_val, tmp_IG = choose_value(train_df, q, total_size)
        tmp_q_val, tmp_IG = choose_value(train_df, q, total_size)
        if tmp_IG >= best_IG:
            best_IG = tmp_IG
            best_q = q
            q_val = tmp_q_val
    return (best_q, q_val)


def create_leaf_with_majority(train_df):
    mask1 = (train_df["diagnosis"] == "B")
    if len(train_df[mask1]) >= (len(train_df) / 2):
        return Leaf("B", data=train_df)
    return Leaf("M", data=train_df)


def get_majority(train_np, total_size):
    if np.count_nonzero(train_np == "M") >= np.count_nonzero(train_np == "B"):
        return "M"
    return "B"


def build_tree(train_df, qualities, classification=None, M=None):
    """Builtd the tree (during training phase).
        Children[0] is if value is bigger (or equal)."""
    # print("M is: ", M, ",  classification is: ", classification)
    if M != None and len(train_df) < M:
        if classification == None:
            print("WTF2!?!?!")
        return Leaf(classification, data=train_df)

    num_of_samples = len(train_df)
    diags = np.array(train_df["diagnosis"])
    classification = get_majority(diags, num_of_samples)

    classes = np.array(train_df["diagnosis"])
    if np.unique(classes).size == 1:
        return Leaf(classification, data=train_df)

    attrb, val = choose_quality(train_df, qualities, num_of_samples)

    children = [None, None]
    mask1 = (train_df[attrb] >= val)
    mask2 = (train_df[attrb] < val)
    first_child_data = train_df[mask1]
    second_child_data = train_df[mask2]
    children[0] = build_tree(first_child_data, qualities, classification, M)
    children[1] = build_tree(second_child_data, qualities, classification, M)

    return Node(attrb, val, children, num_of_samples, data=train_df)


def get_y_for_smaple(sample, built_tree, idx):
    """ Return the lable of sample """
    node = built_tree
    while True:
        if type(node) is Leaf:
            return node.diag
        attr = node.type
        val = node.split_val
        if sample[attr][idx] >= val:
            node = node.children[0]
        else:
            node = node.children[1]


def test(test_df, built_tree, calc_loss=False):
    total = 0
    num_correct = 0
    FP = 0
    FN = 0
    for ind in test_df.index:
        y_hat = get_y_for_smaple(test_df, built_tree, ind)
        if y_hat == test_df["diagnosis"][ind]:
            num_correct += 1
        elif calc_loss == True:
            if y_hat == "B": # If sick but diagnosed healthy
                FN += 1
            else: # If healthy but diagnosed sick
                FP += 1
        total += 1
    if calc_loss == False:
        return num_correct*1./total
    else:
        loss = (0.1*FP + FN)/total
        return loss

# def calc_loss(test_df, built_tree):



def prune_tree(tree, M):
    """ Gets a built tree and prunes it with value M """
    if type(tree) is Leaf:
        return Leaf(tree.diag, tree.data)
    if tree.num_of_samples < M:
        return create_leaf_with_majority(tree.data)

    children = [None, None]
    children[0] = prune_tree(tree.children[0], M)
    children[1] = prune_tree(tree.children[1], M)
    return Node(tree.type, tree.split_val, children, tree.num_of_samples, data=tree.data)


def K_fold_prune_tree(M, train_df, qualities):
    # kf = KFold(n_splits=5, random_state=209530245, shuffle=True)
    kf = KFold(n_splits=5, random_state=123456789, shuffle=True)
    acc_list = []
    for train_index, test_index in kf.split(train_df):
        X_train, X_test = train_df.loc[train_index], train_df.loc[test_index]
        p_tree = build_tree(X_train, qualities, M=M)
        acc = test(X_test, p_tree)
        acc_list.append(acc)
        #print("train: ", train_index)
    print("Acc of M=", M, " is:", acc_list)
    return acc_list


def choose_M(M_list, train_df, qualities):
    # M_list = [0]
    for M in M_list:
        print("Training with M=", M, "...", end="  ")
        acc_list = K_fold_prune_tree(M, train_df, qualities)
        print("The AVG of M=", M, " is:", sum(acc_list)/len(acc_list))


def get_best_M(train_df, qualities):
    # M_list = [0, 2, 5]
    M_list = [1, 2, 3, 5, 8, 16, 30, 50, 80, 120]
    choose_M(M_list, train_df, qualities)


def calc_loss(df, majority):
    """ Gets a df an calculates the loss """
    wrong = 0
    for ind in df.index:
        if majority != df["diagnosis"][ind]:
            wrong += 1
    if majority == "M":
        wrong *= 0.1
    return wrong


def change_type_of_leaf(leaf: Leaf):
    loss1 = calc_loss(leaf.data, "M")
    loss2 = calc_loss(leaf.data, "B")
    if loss1 < loss2:
        if leaf.diag != "M":
            print("AAAAAA")
        leaf.diag = "M"
    else:
        if leaf.diag != "B":
            print("BBBBBB")
        leaf.diag = "B"


def decrease_loss(tree):
    if type(tree) is Leaf:
        change_type_of_leaf(tree)
    else:
        decrease_loss(tree.children[0])
        decrease_loss(tree.children[1])

qualities = [x for x in train_df.keys()]
qualities.remove('diagnosis')
get_best_M(train_df, qualities)
